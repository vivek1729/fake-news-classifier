{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "import cPickle as cpickle\n",
    "import numpy as np\n",
    "import keras.utils\n",
    "import time\n",
    "from keras.callbacks import TensorBoard, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set = pd.read_table('liar_dataset/train.tsv',\n",
    "                         names = [\"label\", \"statement\", \"subject\", \"speaker\", \"job\", \"state\", \"party\", \"venue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>party</th>\n",
       "      <th>venue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>a mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>an interview on CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                          statement  \\\n",
       "0        false  Says the Annies List political group supports ...   \n",
       "1    half-true  When did the decline of coal start? It started...   \n",
       "2  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3        false  Health care reform legislation is likely to ma...   \n",
       "4    half-true  The economic turnaround started at the end of ...   \n",
       "\n",
       "                              subject         speaker                   job  \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "2                      foreign-policy    barack-obama             President   \n",
       "3                         health-care    blog-posting                   NaN   \n",
       "4                        economy,jobs   charlie-crist                   NaN   \n",
       "\n",
       "      state       party                venue  \n",
       "0     Texas  republican             a mailer  \n",
       "1  Virginia    democrat      a floor speech.  \n",
       "2  Illinois    democrat               Denver  \n",
       "3       NaN        none       a news release  \n",
       "4   Florida    democrat  an interview on CNN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File reading is done\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Using pretrained word embeddings (an experiment)\n",
    "#Read glove vectors and get unique words in an array\n",
    "embeddings_index = {}\n",
    "with open('/Users/vivekpradhan/Downloads/glove.6B/glove.6B.100d.txt') as fp:\n",
    "    for line in fp:\n",
    "        values = line.split()\n",
    "        vectors = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[values[0].lower()] = vectors\n",
    "print \"File reading is done\"\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_set = pd.read_table('liar_dataset/valid.tsv',\n",
    "                         names = [\"label\", \"statement\", \"subject\", \"speaker\", \"job\", \"state\", \"party\", \"venue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('liar_dataset/test.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test set 1283\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "test_data = []\n",
    "with open('liar_dataset/test.tsv') as test_fp: \n",
    "    for line in test_fp:\n",
    "        line = line.strip('\\n')\n",
    "        test_item = line.split('\\t')\n",
    "        test_data.append(test_item)\n",
    "        counter += 1\n",
    "print 'Length of test set '+str(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Building a wall on the U.S.-Mexico border will take literally years.',\n",
       " 'immigration',\n",
       " 'rick-perry',\n",
       " 'Governor',\n",
       " 'Texas',\n",
       " 'republican',\n",
       " 'Radio interview']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = pd.DataFrame(test_data,columns=[\"statement\", \"subject\", \"speaker\", \"job\", \"state\", \"party\", \"venue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1283 entries, 0 to 1282\n",
      "Data columns (total 7 columns):\n",
      "statement    1283 non-null object\n",
      "subject      1283 non-null object\n",
      "speaker      1283 non-null object\n",
      "job          1283 non-null object\n",
      "state        1283 non-null object\n",
      "party        1283 non-null object\n",
      "venue        1283 non-null object\n",
      "dtypes: object(7)\n",
      "memory usage: 70.2+ KB\n"
     ]
    }
   ],
   "source": [
    "test_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_dict = {'pants-fire':0,'false':1,'barely-true':2,'half-true':3,'mostly-true':4,'true':5}\n",
    "label_reverse_arr = ['pants-fire','false','barely-true','half-true','mostly-true','true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>party</th>\n",
       "      <th>venue</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>barely-true</td>\n",
       "      <td>We have less Americans working now than in the...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>vicky-hartzler</td>\n",
       "      <td>U.S. Representative</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>republican</td>\n",
       "      <td>an interview with ABC17 News</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pants-fire</td>\n",
       "      <td>When Obama was sworn into office, he DID NOT u...</td>\n",
       "      <td>obama-birth-certificate,religion</td>\n",
       "      <td>chain-email</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>false</td>\n",
       "      <td>Says Having organizations parading as being so...</td>\n",
       "      <td>campaign-finance,congress,taxes</td>\n",
       "      <td>earl-blumenauer</td>\n",
       "      <td>U.S. representative</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>democrat</td>\n",
       "      <td>a U.S. Ways and Means hearing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                          statement  \\\n",
       "0  barely-true  We have less Americans working now than in the...   \n",
       "1   pants-fire  When Obama was sworn into office, he DID NOT u...   \n",
       "2        false  Says Having organizations parading as being so...   \n",
       "\n",
       "                            subject          speaker                  job  \\\n",
       "0                      economy,jobs   vicky-hartzler  U.S. Representative   \n",
       "1  obama-birth-certificate,religion      chain-email                  NaN   \n",
       "2   campaign-finance,congress,taxes  earl-blumenauer  U.S. representative   \n",
       "\n",
       "      state       party                          venue  label_id  \n",
       "0  Missouri  republican   an interview with ABC17 News         2  \n",
       "1       NaN        none                            NaN         0  \n",
       "2    Oregon    democrat  a U.S. Ways and Means hearing         1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_one_hot(x):\n",
    "    return keras.utils.to_categorical(label_dict[x],num_classes=6)\n",
    "data_set['label_id'] = data_set['label'].apply(lambda x: label_dict[x])\n",
    "val_set['label_id'] = val_set['label'].apply(lambda x: label_dict[x])\n",
    "val_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'marco-rubio': 8, 'mitt-romney': 3, 'chain-email': 7, 'bernie-s': 11, 'newt-gingrich': 15, 'rick-perry': 6, 'hillary-clinton': 2, 'chris-christie': 12, 'facebook-posts': 13, 'charlie-crist': 14, 'john-mccain': 5, 'barack-obama': 0, 'joe-biden': 17, 'blog-posting': 18, 'paul-ryan': 19, 'jeb-bush': 16, 'donald-trump': 1, 'scott-walker': 4, 'rick-scott': 9, 'ted-cruz': 10}\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "speakers = ['barack-obama', 'donald-trump', 'hillary-clinton', 'mitt-romney', \n",
    "            'scott-walker', 'john-mccain', 'rick-perry', 'chain-email', \n",
    "            'marco-rubio', 'rick-scott', 'ted-cruz', 'bernie-s', 'chris-christie', \n",
    "            'facebook-posts', 'charlie-crist', 'newt-gingrich', 'jeb-bush', \n",
    "            'joe-biden', 'blog-posting','paul-ryan']\n",
    "speaker_dict = {}\n",
    "for cnt,speaker in enumerate(speakers):\n",
    "    speaker_dict[speaker] = cnt\n",
    "print speaker_dict\n",
    "def map_speaker(speaker):\n",
    "    if isinstance(speaker, basestring):\n",
    "        speaker = speaker.lower()\n",
    "        matches = [s for s in speakers if s in speaker]\n",
    "        if len(matches) > 0:\n",
    "            return speaker_dict[matches[0]] #Return index of first match\n",
    "        else:\n",
    "            return len(speakers)\n",
    "    else:\n",
    "        return len(speakers) #Nans or un-string data goes here.\n",
    "data_set['speaker_id'] = data_set['speaker'].apply(map_speaker)\n",
    "val_set['speaker_id'] = val_set['speaker'].apply(map_speaker)\n",
    "print len(speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_set['job'].value_counts()[:10]\n",
    "job_list = ['president', 'u.s. senator', 'governor', 'president-elect', 'presidential candidate', \n",
    "            'u.s. representative', 'state senator', 'attorney', 'state representative', 'congress']\n",
    "\n",
    "job_dict = {'president':0, 'u.s. senator':1, 'governor':2, 'president-elect':3, 'presidential candidate':4, \n",
    "            'u.s. representative':5, 'state senator':6, 'attorney':7, 'state representative':8, 'congress':9}\n",
    "#Possible groupings could be (11 groups)\n",
    "#president, us senator, governor(contains governor), president-elect, presidential candidate, us representative,\n",
    "#state senator, attorney, state representative, congress (contains congressman or congresswoman), rest\n",
    "def map_job(job):\n",
    "    if isinstance(job, basestring):\n",
    "        job = job.lower()\n",
    "        matches = [s for s in job_list if s in job]\n",
    "        if len(matches) > 0:\n",
    "            return job_dict[matches[0]] #Return index of first match\n",
    "        else:\n",
    "            return 10 #This maps any other job to index 10\n",
    "    else:\n",
    "        return 10 #Nans or un-string data goes here.\n",
    "data_set['job_id'] = data_set['job'].apply(map_job)\n",
    "val_set['job_id'] = val_set['job'].apply(map_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set['party'].value_counts()\n",
    "#Possible groupings (6 groups)\n",
    "#Hyper param -> num_party\n",
    "party_dict = {'republican':0,'democrat':1,'none':2,'organization':3,'newsmaker':4}\n",
    "#default index for rest party is 5\n",
    "def map_party(party):\n",
    "    if party in party_dict:\n",
    "        return party_dict[party]\n",
    "    else:\n",
    "        return 5\n",
    "data_set['party_id'] = data_set['party'].apply(map_party)\n",
    "val_set['party_id'] = val_set['party'].apply(map_party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print data_set['state'].value_counts()[0:50]\n",
    "#Possible groupings (50 groups + 1 for rest)\n",
    "states = ['Alabama','Alaska','Arizona','Arkansas','California','Colorado',\n",
    "         'Connecticut','Delaware','Florida','Georgia','Hawaii','Idaho', \n",
    "         'Illinois','Indiana','Iowa','Kansas','Kentucky','Louisiana',\n",
    "         'Maine' 'Maryland','Massachusetts','Michigan','Minnesota',\n",
    "         'Mississippi', 'Missouri','Montana','Nebraska','Nevada',\n",
    "         'New Hampshire','New Jersey','New Mexico','New York',\n",
    "         'North Carolina','North Dakota','Ohio',    \n",
    "         'Oklahoma','Oregon','Pennsylvania','Rhode Island',\n",
    "         'South  Carolina','South Dakota','Tennessee','Texas','Utah',\n",
    "         'Vermont','Virginia','Washington','West Virginia',\n",
    "         'Wisconsin','Wyoming']\n",
    "#states_dict = {}\n",
    "#i = 0\n",
    "#for state in states:\n",
    "#    state_key = state.lower()\n",
    "#    states_dict[state_key] = i\n",
    "#    i += 1\n",
    "#print len(states_dict.keys())\n",
    "\n",
    "states_dict = {'wyoming': 48, 'colorado': 5, 'washington': 45, 'hawaii': 10, 'tennessee': 40, 'wisconsin': 47, 'nevada': 26, 'north dakota': 32, 'mississippi': 22, 'south dakota': 39, 'new jersey': 28, 'oklahoma': 34, 'delaware': 7, 'minnesota': 21, 'north carolina': 31, 'illinois': 12, 'new york': 30, 'arkansas': 3, 'west virginia': 46, 'indiana': 13, 'louisiana': 17, 'idaho': 11, 'south  carolina': 38, 'arizona': 2, 'iowa': 14, 'mainemaryland': 18, 'michigan': 20, 'kansas': 15, 'utah': 42, 'virginia': 44, 'oregon': 35, 'connecticut': 6, 'montana': 24, 'california': 4, 'massachusetts': 19, 'rhode island': 37, 'vermont': 43, 'georgia': 9, 'pennsylvania': 36, 'florida': 8, 'alaska': 1, 'kentucky': 16, 'nebraska': 25, 'new hampshire': 27, 'texas': 41, 'missouri': 23, 'ohio': 33, 'alabama': 0, 'new mexico': 29}\n",
    "def map_state(state):\n",
    "    if isinstance(state, basestring):\n",
    "        state = state.lower()\n",
    "        if state in states_dict:\n",
    "            return states_dict[state]\n",
    "        else:\n",
    "            if 'washington' in state:\n",
    "                return states_dict['washington']\n",
    "            else:\n",
    "                return 50 #This maps any other location to index 50\n",
    "    else:\n",
    "        return 50 #Nans or un-string data goes here.\n",
    "data_set['state_id'] = data_set['state'].apply(map_state)\n",
    "val_set['state_id'] = val_set['state'].apply(map_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set['subject'].value_counts()[0:5]\n",
    "#Possible groups (14)\n",
    "subject_list = ['health','tax','immigration','election','education',\n",
    "'candidates-biography','economy','gun','jobs','federal-budget','energy','abortion','foreign-policy']\n",
    "\n",
    "subject_dict = {'health':0,'tax':1,'immigration':2,'election':3,'education':4,\n",
    "'candidates-biography':5,'economy':6,'gun':7,'jobs':8,'federal-budget':9,'energy':10,'abortion':11,'foreign-policy':12}\n",
    "#health-care,taxes,immigration,elections,education,candidates-biography,guns,\n",
    "#economy&jobs ,federal-budget,energy,abortion,foreign-policy,state-budget, rest\n",
    "#Economy & Jobs is bundled together, because it occurs together\n",
    "def map_subject(subject):\n",
    "    if isinstance(subject, basestring):\n",
    "        subject = subject.lower()\n",
    "        matches = [s for s in subject_list if s in subject]\n",
    "        if len(matches) > 0:\n",
    "            return subject_dict[matches[0]] #Return index of first match\n",
    "        else:\n",
    "            return 13 #This maps any other subject to index 13\n",
    "    else:\n",
    "        return 13 #Nans or un-string data goes here.\n",
    "\n",
    "data_set['subject_id'] = data_set['subject'].apply(map_subject)\n",
    "val_set['subject_id'] = val_set['subject'].apply(map_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set['venue'].value_counts()[0:15]\n",
    "\n",
    "venue_list = ['news release','interview','tv','radio',\n",
    "              'campaign','news conference','press conference','press release',\n",
    "              'tweet','facebook','email']\n",
    "venue_dict = {'news release':0,'interview':1,'tv':2,'radio':3,\n",
    "              'campaign':4,'news conference':5,'press conference':6,'press release':7,\n",
    "              'tweet':8,'facebook':9,'email':10}\n",
    "def map_venue(venue):\n",
    "    if isinstance(venue, basestring):\n",
    "        venue = venue.lower()\n",
    "        matches = [s for s in venue_list if s in venue]\n",
    "        if len(matches) > 0:\n",
    "            return venue_dict[matches[0]] #Return index of first match\n",
    "        else:\n",
    "            return 11 #This maps any other venue to index 11\n",
    "    else:\n",
    "        return 11 #Nans or un-string data goes here.\n",
    "#possibe groups (12)\n",
    "#news release, interview, tv (television), radio, campaign, news conference, press conference, press release,\n",
    "#tweet, facebook, email, rest\n",
    "data_set['venue_id'] = data_set['venue'].apply(map_venue)\n",
    "val_set['venue_id'] = val_set['venue'].apply(map_venue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocab dict from pickle file\n"
     ]
    }
   ],
   "source": [
    "#Tokenize statement and vocab test\n",
    "vocab_dict = {}\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "if not os.path.exists('vocab.p'):\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(data_set['statement'])\n",
    "    vocab_dict = t.word_index\n",
    "    cpickle.dump( t.word_index, open( \"vocab.p\", \"wb\" ))\n",
    "    print 'Vocab dict is created'\n",
    "    print 'Saved vocab dict to pickle file'\n",
    "else:\n",
    "    print 'Loading vocab dict from pickle file'\n",
    "    vocab_dict = cpickle.load(open(\"vocab.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get all preprocessing done for test data\n",
    "test_set['job_id'] = test_set['job'].apply(map_job) #Job\n",
    "test_set['party_id'] = test_set['party'].apply(map_party) #Party\n",
    "test_set['state_id'] = test_set['state'].apply(map_state) #State\n",
    "test_set['subject_id'] = test_set['subject'].apply(map_subject) #Subject\n",
    "test_set['venue_id'] = test_set['venue'].apply(map_venue) #Venue\n",
    "test_set['speaker_id'] = test_set['speaker'].apply(map_speaker) #Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>party</th>\n",
       "      <th>venue</th>\n",
       "      <th>job_id</th>\n",
       "      <th>party_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>venue_id</th>\n",
       "      <th>speaker_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Building a wall on the U.S.-Mexico border will...</td>\n",
       "      <td>immigration</td>\n",
       "      <td>rick-perry</td>\n",
       "      <td>Governor</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>Radio interview</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wisconsin is on pace to double the number of l...</td>\n",
       "      <td>jobs</td>\n",
       "      <td>katrina-shankland</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>democrat</td>\n",
       "      <td>a news conference</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Says John McCain has done nothing to help the ...</td>\n",
       "      <td>military,veterans,voting-record</td>\n",
       "      <td>donald-trump</td>\n",
       "      <td>President-Elect</td>\n",
       "      <td>New York</td>\n",
       "      <td>republican</td>\n",
       "      <td>comments on ABC's This Week.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement  \\\n",
       "0  Building a wall on the U.S.-Mexico border will...   \n",
       "1  Wisconsin is on pace to double the number of l...   \n",
       "2  Says John McCain has done nothing to help the ...   \n",
       "\n",
       "                           subject            speaker                   job  \\\n",
       "0                      immigration         rick-perry              Governor   \n",
       "1                             jobs  katrina-shankland  State representative   \n",
       "2  military,veterans,voting-record       donald-trump       President-Elect   \n",
       "\n",
       "       state       party                         venue  job_id  party_id  \\\n",
       "0      Texas  republican               Radio interview       2         0   \n",
       "1  Wisconsin    democrat             a news conference       8         1   \n",
       "2   New York  republican  comments on ABC's This Week.       0         0   \n",
       "\n",
       "   state_id  subject_id  venue_id  speaker_id  \n",
       "0        41           2         1           6  \n",
       "1        47           8         5          20  \n",
       "2        30          13        11           1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To access particular word_index. Just load these.\n",
    "#To read a word in a sentence use keras tokenizer again, coz easy\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "#text = text_to_word_sequence(data_set['statement'][0])\n",
    "#print text\n",
    "#val = [vocab_dict[t] for t in text]\n",
    "#print val\n",
    "\n",
    "def pre_process_statement(statement):\n",
    "    text = text_to_word_sequence(statement)\n",
    "    val = [0] * 10\n",
    "    val = [vocab_dict[t] for t in text if t in vocab_dict] #Replace unk words with 0 index\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating embedding matrix to feed in embeddings directly bruv\n",
    "num_words = len(vocab_dict) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in vocab_dict.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I have reset embeddings_index since it would take a lot of memory\n",
    "embeddings_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameter definitions\n",
    "vocab_length = len(vocab_dict.keys())\n",
    "hidden_size = 100 #Has to be same as EMBEDDING_DIM\n",
    "lstm_size = 100\n",
    "num_steps = 25\n",
    "num_epochs = 30\n",
    "batch_size = 40\n",
    "#Hyperparams for CNN\n",
    "kernel_sizes = [2,5,8]\n",
    "filter_size = 128\n",
    "#Meta data related hyper params\n",
    "num_party = 6\n",
    "num_state = 51\n",
    "num_venue = 12\n",
    "num_job = 11\n",
    "num_sub = 14\n",
    "num_speaker = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load data and pad sequences to prepare training, validation and test data\n",
    "data_set['word_ids'] = data_set['statement'].apply(pre_process_statement)\n",
    "val_set['word_ids'] = val_set['statement'].apply(pre_process_statement)\n",
    "test_set['word_ids'] = test_set['statement'].apply(pre_process_statement)\n",
    "X_train = data_set['word_ids']\n",
    "Y_train = data_set['label_id']\n",
    "X_val = val_set['word_ids']\n",
    "Y_val = val_set['label_id']\n",
    "X_test = test_set['word_ids']\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=num_steps, padding='post',truncating='post')\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes=6)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=num_steps, padding='post',truncating='post')\n",
    "Y_val = keras.utils.to_categorical(Y_val, num_classes=6)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=num_steps, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Meta data preparation\n",
    "a = keras.utils.to_categorical(data_set['party_id'], num_classes=num_party)\n",
    "b = keras.utils.to_categorical(data_set['state_id'], num_classes=num_state)\n",
    "c = keras.utils.to_categorical(data_set['venue_id'], num_classes=num_venue)\n",
    "d = keras.utils.to_categorical(data_set['job_id'], num_classes=num_job)\n",
    "e = keras.utils.to_categorical(data_set['subject_id'], num_classes=num_sub)\n",
    "f = keras.utils.to_categorical(data_set['speaker_id'], num_classes=num_speaker)\n",
    "X_train_meta = np.hstack((a,b,c,d,e,f))#concat a and b\n",
    "a_val = keras.utils.to_categorical(val_set['party_id'], num_classes=num_party)\n",
    "b_val = keras.utils.to_categorical(val_set['state_id'], num_classes=num_state)\n",
    "c_val = keras.utils.to_categorical(val_set['venue_id'], num_classes=num_venue)\n",
    "d_val = keras.utils.to_categorical(val_set['job_id'], num_classes=num_job)\n",
    "e_val = keras.utils.to_categorical(val_set['subject_id'], num_classes=num_sub)\n",
    "f_val = keras.utils.to_categorical(val_set['speaker_id'], num_classes=num_speaker)\n",
    "X_val_meta = np.hstack((a_val,b_val,c_val,d_val,e_val,f_val))#concat a_val and b_val\n",
    "a_test = keras.utils.to_categorical(test_set['party_id'], num_classes=num_party)\n",
    "b_test = keras.utils.to_categorical(test_set['state_id'], num_classes=num_state)\n",
    "c_test = keras.utils.to_categorical(test_set['venue_id'], num_classes=num_venue)\n",
    "d_test = keras.utils.to_categorical(test_set['job_id'], num_classes=num_job)\n",
    "e_test = keras.utils.to_categorical(test_set['subject_id'], num_classes=num_sub)\n",
    "f_test = keras.utils.to_categorical(test_set['speaker_id'], num_classes=num_speaker)\n",
    "X_test_meta = np.hstack((a_test,b_test,c_test,d_test,e_test,f_test))#concat all test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1284, 6)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10240, 115)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,LSTM,Conv1D,GlobalMaxPool1D,Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Keras LSTM Model (defining sequential model simple)\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_length+1, hidden_size, input_length=num_steps))\n",
    "model.add(LSTM(hidden_size))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a complex model. Adding meta data features to the model (LSTM Based)\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "x = Embedding(vocab_length+1,EMBEDDING_DIM,weights=[embedding_matrix],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "#x = Embedding(output_dim=hidden_size, input_dim=vocab_length+1, input_length=num_steps)(statement_input) #Train embeddings from scratch\n",
    "lstm_in = LSTM(lstm_size,dropout=0.2)(x)\n",
    "meta_input = Input(shape=(X_train_meta.shape[1],), name='aux_input')\n",
    "#x_em = Embedding(output_dim=hidden_size, input_dim=2, input_length=X_train_meta.shape[1])(meta_input)\n",
    "x_meta = Dense(64, activation='relu')(meta_input)\n",
    "#x_meta = Dense(56, activation='relu')(x_meta)\n",
    "x = keras.layers.concatenate([lstm_in, x_meta])\n",
    "#x = Dense(128, activation='relu')(x) #Add some density\n",
    "#x = Dense(64, activation='relu')(x) #Add some density\n",
    "#x = Dense(32, activation='relu')(x) #Add some density\n",
    "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
    "model = Model(inputs=[statement_input, meta_input], outputs=[main_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining a complex model. Adding meta data features to the model (CNN Based)\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "kernel_arr = []\n",
    "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "#x = Embedding(vocab_length+1,EMBEDDING_DIM,weights=[embedding_matrix],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "x = Embedding(output_dim=hidden_size, input_dim=vocab_length+1, input_length=num_steps)(statement_input) #Train embeddings from scratch\n",
    "\n",
    "for kernel in kernel_sizes:\n",
    "    x_1 = Conv1D(filters=filter_size,kernel_size=kernel)(x)\n",
    "    x_1 = GlobalMaxPool1D()(x_1)\n",
    "    kernel_arr.append(x_1)\n",
    "conv_in = keras.layers.concatenate(kernel_arr)\n",
    "conv_in = Dropout(0.6)(conv_in)\n",
    "conv_in = Dense(128, activation='relu')(conv_in)\n",
    "\n",
    "#Meta input\n",
    "meta_input = Input(shape=(X_train_meta.shape[1],), name='aux_input')\n",
    "#x_em = Embedding(output_dim=hidden_size, input_dim=2, input_length=X_train_meta.shape[1])(meta_input)\n",
    "x_meta = Dense(64, activation='relu')(meta_input)\n",
    "#x_meta = Dense(56, activation='relu')(x_meta)\n",
    "x = keras.layers.concatenate([conv_in, x_meta])\n",
    "#x = Dense(128, activation='relu')(x) #Add some density\n",
    "#x = Dense(64, activation='relu')(x) #Add some density\n",
    "#x = Dense(32, activation='relu')(x) #Add some density\n",
    "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
    "model = Model(inputs=[statement_input, meta_input], outputs=[main_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"377pt\" viewBox=\"0.00 0.00 630.42 377.00\" width=\"630pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 373)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-373 626.4199,-373 626.4199,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 4575053648 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>4575053648</title>\n",
       "<polygon fill=\"none\" points=\"21.3931,-324.5 21.3931,-368.5 303.0659,-368.5 303.0659,-324.5 21.3931,-324.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"96.0708\" y=\"-342.3\">main_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"170.7485,-324.5 170.7485,-368.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.583\" y=\"-353.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"170.7485,-346.5 226.4175,-346.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.583\" y=\"-331.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"226.4175,-324.5 226.4175,-368.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264.7417\" y=\"-353.3\">(None, 25)</text>\n",
       "<polyline fill=\"none\" points=\"226.4175,-346.5 303.0659,-346.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264.7417\" y=\"-331.3\">(None, 25)</text>\n",
       "</g>\n",
       "<!-- 4575024720 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>4575024720</title>\n",
       "<polygon fill=\"none\" points=\"0,-243.5 0,-287.5 324.459,-287.5 324.459,-243.5 0,-243.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82.0708\" y=\"-261.3\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"164.1416,-243.5 164.1416,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191.9761\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"164.1416,-265.5 219.8105,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191.9761\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"219.8105,-243.5 219.8105,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"272.1348\" y=\"-272.3\">(None, 25)</text>\n",
       "<polyline fill=\"none\" points=\"219.8105,-265.5 324.459,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"272.1348\" y=\"-250.3\">(None, 25, 100)</text>\n",
       "</g>\n",
       "<!-- 4575053648&#45;&gt;4575024720 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>4575053648-&gt;4575024720</title>\n",
       "<path d=\"M162.2295,-324.3664C162.2295,-316.1516 162.2295,-306.6579 162.2295,-297.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"165.7296,-297.6068 162.2295,-287.6068 158.7296,-297.6069 165.7296,-297.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4376793040 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>4376793040</title>\n",
       "<polygon fill=\"none\" points=\"45.6484,-162.5 45.6484,-206.5 304.8105,-206.5 304.8105,-162.5 45.6484,-162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.0708\" y=\"-180.3\">lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"144.4932,-162.5 144.4932,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172.3276\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"144.4932,-184.5 200.1621,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172.3276\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"200.1621,-162.5 200.1621,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.4863\" y=\"-191.3\">(None, 25, 100)</text>\n",
       "<polyline fill=\"none\" points=\"200.1621,-184.5 304.8105,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.4863\" y=\"-169.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 4575024720&#45;&gt;4376793040 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>4575024720-&gt;4376793040</title>\n",
       "<path d=\"M165.7818,-243.3664C167.1002,-235.1516 168.6239,-225.6579 170.0576,-216.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"173.5525,-217.0351 171.6815,-206.6068 166.641,-215.9258 173.5525,-217.0351\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4575055760 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>4575055760</title>\n",
       "<polygon fill=\"none\" points=\"342.0391,-243.5 342.0391,-287.5 622.4199,-287.5 622.4199,-243.5 342.0391,-243.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.8271\" y=\"-261.3\">aux_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"483.6152,-243.5 483.6152,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"511.4497\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"483.6152,-265.5 539.2842,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"511.4497\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"539.2842,-243.5 539.2842,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"580.8521\" y=\"-272.3\">(None, 115)</text>\n",
       "<polyline fill=\"none\" points=\"539.2842,-265.5 622.4199,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"580.8521\" y=\"-250.3\">(None, 115)</text>\n",
       "</g>\n",
       "<!-- 4358458768 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>4358458768</title>\n",
       "<polygon fill=\"none\" points=\"347.7012,-162.5 347.7012,-206.5 590.7578,-206.5 590.7578,-162.5 347.7012,-162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399.8271\" y=\"-180.3\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"451.9531,-162.5 451.9531,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479.7876\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"451.9531,-184.5 507.6221,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479.7876\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"507.6221,-162.5 507.6221,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"549.1899\" y=\"-191.3\">(None, 115)</text>\n",
       "<polyline fill=\"none\" points=\"507.6221,-184.5 590.7578,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"548.9463\" y=\"-169.3\">(None, 64)</text>\n",
       "</g>\n",
       "<!-- 4575055760&#45;&gt;4358458768 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>4575055760-&gt;4358458768</title>\n",
       "<path d=\"M478.6772,-243.3664C477.3588,-235.1516 475.8351,-225.6579 474.4014,-216.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"477.818,-215.9258 472.7775,-206.6068 470.9065,-217.0351 477.818,-215.9258\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4589018256 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>4589018256</title>\n",
       "<polygon fill=\"none\" points=\"127.7651,-81.5 127.7651,-125.5 516.6938,-125.5 516.6938,-81.5 127.7651,-81.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214.0845\" y=\"-99.3\">concatenate_1: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"300.4038,-81.5 300.4038,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"328.2383\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"300.4038,-103.5 356.0728,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"328.2383\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"356.0728,-81.5 356.0728,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436.3833\" y=\"-110.3\">[(None, 100), (None, 64)]</text>\n",
       "<polyline fill=\"none\" points=\"356.0728,-103.5 516.6938,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"435.897\" y=\"-88.3\">(None, 164)</text>\n",
       "</g>\n",
       "<!-- 4376793040&#45;&gt;4589018256 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>4376793040-&gt;4589018256</title>\n",
       "<path d=\"M215.3979,-162.3664C233.223,-152.5444 254.3663,-140.894 273.1202,-130.5602\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"275.0404,-133.4984 282.1097,-125.6068 271.6621,-127.3675 275.0404,-133.4984\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4358458768&#45;&gt;4589018256 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>4358458768-&gt;4589018256</title>\n",
       "<path d=\"M429.0611,-162.3664C411.236,-152.5444 390.0927,-140.894 371.3388,-130.5602\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"372.7968,-127.3675 362.3493,-125.6068 369.4186,-133.4984 372.7968,-127.3675\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4603546512 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>4603546512</title>\n",
       "<polygon fill=\"none\" points=\"187.9966,-.5 187.9966,-44.5 456.4624,-44.5 456.4624,-.5 187.9966,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.5708\" y=\"-18.3\">main_output: Dense</text>\n",
       "<polyline fill=\"none\" points=\"317.145,-.5 317.145,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"344.9795\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"317.145,-22.5 372.814,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"344.9795\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"372.814,-.5 372.814,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"414.6382\" y=\"-29.3\">(None, 164)</text>\n",
       "<polyline fill=\"none\" points=\"372.814,-22.5 456.4624,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"414.6382\" y=\"-7.3\">(None, 6)</text>\n",
       "</g>\n",
       "<!-- 4589018256&#45;&gt;4603546512 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>4589018256-&gt;4603546512</title>\n",
       "<path d=\"M322.2295,-81.3664C322.2295,-73.1516 322.2295,-63.6579 322.2295,-54.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"325.7296,-54.6068 322.2295,-44.6068 318.7296,-54.6069 325.7296,-54.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualize model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_lstm.png', show_shapes=True, show_layer_names=True)\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model,show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 25, 100)      1237800     main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "aux_input (InputLayer)          (None, 115)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  (None, 100)          80400       embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 64)           7424        aux_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 164)          0           lstm_13[0][0]                    \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 6)            990         concatenate_13[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,326,614\n",
      "Trainable params: 88,814\n",
      "Non-trainable params: 1,237,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Compile model and print summary\n",
    "#Define specific optimizer to counter over-fitting\n",
    "sgd = optimizers.SGD(lr=0.025, clipvalue=0.3, nesterov=True)\n",
    "adam = optimizers.Adam(lr=0.000075, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Start training the sequential model here\n",
    "#model.fit(X_train,Y_train,batch_size=10,epochs=10,verbose=1,validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Output stuff to tensorboard and write csv too. So I will define some callbacks\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('run/training.log')\n",
    "filepath= \"weights.best.hdf5\"\n",
    "#Or use val_loss depending on whatever the heck you want\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10240 samples, validate on 1284 samples\n",
      "Epoch 1/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.7560 - categorical_accuracy: 0.2154 - val_loss: 1.7390 - val_categorical_accuracy: 0.2375\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.23754, saving model to weights.best.hdf5\n",
      "Epoch 2/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.7390 - categorical_accuracy: 0.2282 - val_loss: 1.7252 - val_categorical_accuracy: 0.2438\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.23754 to 0.24377, saving model to weights.best.hdf5\n",
      "Epoch 3/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.7277 - categorical_accuracy: 0.2353 - val_loss: 1.7143 - val_categorical_accuracy: 0.2422\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy did not improve\n",
      "Epoch 4/30\n",
      "10240/10240 [==============================] - 23s 2ms/step - loss: 1.7195 - categorical_accuracy: 0.2458 - val_loss: 1.7047 - val_categorical_accuracy: 0.2640\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.24377 to 0.26402, saving model to weights.best.hdf5\n",
      "Epoch 5/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.7125 - categorical_accuracy: 0.2475 - val_loss: 1.6974 - val_categorical_accuracy: 0.2664\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.26402 to 0.26636, saving model to weights.best.hdf5\n",
      "Epoch 6/30\n",
      "10240/10240 [==============================] - 23s 2ms/step - loss: 1.7074 - categorical_accuracy: 0.2527 - val_loss: 1.6924 - val_categorical_accuracy: 0.2578\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve\n",
      "Epoch 7/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.7027 - categorical_accuracy: 0.2575 - val_loss: 1.6830 - val_categorical_accuracy: 0.2687\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.26636 to 0.26869, saving model to weights.best.hdf5\n",
      "Epoch 8/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6978 - categorical_accuracy: 0.2601 - val_loss: 1.6802 - val_categorical_accuracy: 0.2695\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.26869 to 0.26947, saving model to weights.best.hdf5\n",
      "Epoch 9/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6903 - categorical_accuracy: 0.2630 - val_loss: 1.6815 - val_categorical_accuracy: 0.2757\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.26947 to 0.27570, saving model to weights.best.hdf5\n",
      "Epoch 10/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6842 - categorical_accuracy: 0.2713 - val_loss: 1.6633 - val_categorical_accuracy: 0.2788\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.27570 to 0.27882, saving model to weights.best.hdf5\n",
      "Epoch 11/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6794 - categorical_accuracy: 0.2721 - val_loss: 1.6656 - val_categorical_accuracy: 0.2882\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy improved from 0.27882 to 0.28816, saving model to weights.best.hdf5\n",
      "Epoch 12/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6762 - categorical_accuracy: 0.2733 - val_loss: 1.6657 - val_categorical_accuracy: 0.2835\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy did not improve\n",
      "Epoch 13/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6724 - categorical_accuracy: 0.2761 - val_loss: 1.6596 - val_categorical_accuracy: 0.2850\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy did not improve\n",
      "Epoch 14/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6674 - categorical_accuracy: 0.2832 - val_loss: 1.6557 - val_categorical_accuracy: 0.2882\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy did not improve\n",
      "Epoch 15/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6658 - categorical_accuracy: 0.2794 - val_loss: 1.6653 - val_categorical_accuracy: 0.2843\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy did not improve\n",
      "Epoch 16/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6651 - categorical_accuracy: 0.2804 - val_loss: 1.6693 - val_categorical_accuracy: 0.2874\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy did not improve\n",
      "Epoch 17/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6609 - categorical_accuracy: 0.2833 - val_loss: 1.6530 - val_categorical_accuracy: 0.2913\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy improved from 0.28816 to 0.29128, saving model to weights.best.hdf5\n",
      "Epoch 18/30\n",
      "10240/10240 [==============================] - 25s 2ms/step - loss: 1.6612 - categorical_accuracy: 0.2835 - val_loss: 1.6559 - val_categorical_accuracy: 0.2960\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy improved from 0.29128 to 0.29595, saving model to weights.best.hdf5\n",
      "Epoch 19/30\n",
      "10240/10240 [==============================] - 25s 2ms/step - loss: 1.6577 - categorical_accuracy: 0.2812 - val_loss: 1.6515 - val_categorical_accuracy: 0.2944\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy did not improve\n",
      "Epoch 20/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6565 - categorical_accuracy: 0.2830 - val_loss: 1.6486 - val_categorical_accuracy: 0.2866\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy did not improve\n",
      "Epoch 21/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6534 - categorical_accuracy: 0.2890 - val_loss: 1.6545 - val_categorical_accuracy: 0.2866\n",
      "\n",
      "Epoch 00021: val_categorical_accuracy did not improve\n",
      "Epoch 22/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6521 - categorical_accuracy: 0.2878 - val_loss: 1.6468 - val_categorical_accuracy: 0.2960\n",
      "\n",
      "Epoch 00022: val_categorical_accuracy did not improve\n",
      "Epoch 23/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6490 - categorical_accuracy: 0.2969 - val_loss: 1.6557 - val_categorical_accuracy: 0.2874\n",
      "\n",
      "Epoch 00023: val_categorical_accuracy did not improve\n",
      "Epoch 24/30\n",
      "10240/10240 [==============================] - 24s 2ms/step - loss: 1.6481 - categorical_accuracy: 0.2960 - val_loss: 1.6460 - val_categorical_accuracy: 0.2936\n",
      "\n",
      "Epoch 00024: val_categorical_accuracy did not improve\n",
      "Epoch 25/30\n",
      "10240/10240 [==============================] - 23s 2ms/step - loss: 1.6470 - categorical_accuracy: 0.2958 - val_loss: 1.6513 - val_categorical_accuracy: 0.2991\n",
      "\n",
      "Epoch 00025: val_categorical_accuracy improved from 0.29595 to 0.29907, saving model to weights.best.hdf5\n",
      "Epoch 26/30\n",
      "10240/10240 [==============================] - 23s 2ms/step - loss: 1.6434 - categorical_accuracy: 0.2971 - val_loss: 1.6567 - val_categorical_accuracy: 0.2889\n",
      "\n",
      "Epoch 00026: val_categorical_accuracy did not improve\n",
      "Epoch 27/30\n",
      "10240/10240 [==============================] - 22s 2ms/step - loss: 1.6430 - categorical_accuracy: 0.2979 - val_loss: 1.6507 - val_categorical_accuracy: 0.2944\n",
      "\n",
      "Epoch 00027: val_categorical_accuracy did not improve\n",
      "Epoch 28/30\n",
      "10240/10240 [==============================] - 23s 2ms/step - loss: 1.6372 - categorical_accuracy: 0.2997 - val_loss: 1.6446 - val_categorical_accuracy: 0.2835\n",
      "\n",
      "Epoch 00028: val_categorical_accuracy did not improve\n",
      "Epoch 29/30\n",
      "10240/10240 [==============================] - 23s 2ms/step - loss: 1.6401 - categorical_accuracy: 0.2988 - val_loss: 1.6502 - val_categorical_accuracy: 0.2897\n",
      "\n",
      "Epoch 00029: val_categorical_accuracy did not improve\n",
      "Epoch 30/30\n",
      "10240/10240 [==============================] - 23s 2ms/step - loss: 1.6371 - categorical_accuracy: 0.2971 - val_loss: 1.6525 - val_categorical_accuracy: 0.2889\n",
      "\n",
      "Epoch 00030: val_categorical_accuracy did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x130d18710>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start training the functional model here\n",
    "\n",
    "model.fit({'main_input': X_train, 'aux_input': X_train_meta},\n",
    "          {'main_output': Y_train},epochs=num_epochs, batch_size=batch_size,\n",
    "          validation_data=({'main_input': X_val, 'aux_input': X_val_meta},{'main_output': Y_val}),\n",
    "         callbacks=[tb,csv_logger,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Accuracy result for basic LSTM with just word embeddings and time steps 25\n",
    "#84s 8ms/step - loss: 1.0017 - categorical_accuracy: 0.6376 - val_loss: 2.2503 - val_categorical_accuracy: 0.2298\n",
    "'''\n",
    "hidden_size = 50\n",
    "num_steps = 25\n",
    "num_epochs = 20\n",
    "batch_size = 20\n",
    "'''\n",
    "#Accuracy result for basic LSTM with word embeddings, party and state infor and time steps 25\n",
    "#42s 4ms/step - loss: 0.5065 - categorical_accuracy: 0.8275 - val_loss: 3.2041 - val_categorical_accuracy: 0.2118\n",
    "\n",
    "'''\n",
    "hidden_size = 128\n",
    "num_steps = 25\n",
    "num_epochs = 32\n",
    "batch_size = 32\n",
    "'''\n",
    "#Accuracy result for basic LSTM with word embeddings, party,state,venue,job,subject and epochs 32\n",
    "#33s 3ms/step - loss: 0.1056 - categorical_accuracy: 0.9640 - val_loss: 6.3742 - val_categorical_accuracy: 0.2399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#Load a pre-trained model if any\n",
    "model1 = load_model('weights.best_lstm1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283/1283 [==============================] - 1s 554us/step\n"
     ]
    }
   ],
   "source": [
    "#Make predictions on test set\n",
    "preds = model1.predict([X_test,X_test_meta], batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283\n",
      "false\n"
     ]
    }
   ],
   "source": [
    "print len(preds)\n",
    "print label_reverse_arr[np.argmax(preds[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283\n",
      "Predictions written to file\n"
     ]
    }
   ],
   "source": [
    "#Write data to file\n",
    "vf = open('predictions.txt', 'w+')\n",
    "counter = 0\n",
    "for pred in preds:  \n",
    "    line_string = label_reverse_arr[np.argmax(pred)]\n",
    "    vf.write(line_string+'\\n')\n",
    "    counter += 1\n",
    "print counter\n",
    "print \"Predictions written to file\"\n",
    "vf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStrategies to improve\\n1. vstack meta data. Make it a matrix instead of 1 d vector\\n2. Use pretrained word embeddings\\n3. Intuition about hyperparams for CNN, dropout ~ 0.7, batch size ~ 50, \\nsgd or adam not sure what params best work.. But for sgd ~ 0.0025 and for adam ~ 0.00006\\n'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Strategies to improve\n",
    "1. vstack meta data. Make it a matrix instead of 1 d vector\n",
    "2. Use pretrained word embeddings\n",
    "3. Intuition about hyperparams for CNN, dropout ~ 0.7, batch size ~ 50, \n",
    "sgd or adam not sure what params best work.. But for sgd ~ 0.0025 and for adam ~ 0.00006\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
